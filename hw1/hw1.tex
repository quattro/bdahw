\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm,amssymb,amsopn}

\def\E{\mathbb{E}}
\def\N{\mathcal{N}}
\def\I{\mathbb{I}}
\def\Var{\text{Var}}

\newtheorem{theorem}{Theorem}

\begin{document}
\title{CM229: Homework 1}
\author{Nick Mancuso}
\maketitle

\section*{1 (a)}
Let $N_x = \sum_i^n x_i$ denote the number of ``successes'' observed. We can write the
likelihood of $p$ as \[ \mathcal{L}(p| X) = {n \choose N_x} p^{N_x} (1 - p)^{n - N_x}.\] In order
to find the ML-estimate of the likelihood we perform the equivalent operation of maximizing the
log-likelihood, defined as \[\ell(p | X) = N_x \log p + (n - N_x) \log (1 - p) + O(1),\] whose
derivative is given by, \[ \frac{d \ell}{d p} = \frac{N_x}{p} - \frac{n - N_x}{1 - p}.\] Setting
equal to 0 and solving we see,
\begin{align*}
    \frac{N_x - pn}{p(1 - p)} = 0 \Leftrightarrow 
    \frac{N_x}{n} = p \Leftrightarrow 
    \hat{p} = \frac{\sum_i^n x_i}{n}.
\end{align*}

\section*{1 (b)}
The second derivative of the log-likelihood is given as, 
\[ \frac{d^2 \ell}{d p^2} = \frac{-N_x}{p^2} - \frac{n - N_x}{(1 - p)^2}.\] Therefore, we can
compute the Fisher Information as
\begin{align*}
    I_n(p) = -\E\left[\frac{-N_x}{p^2} - \frac{n - N_x}{(1 - p)^2} \middle\rvert p \right] = 
    \E\left[\frac{N_x}{p^2} + \frac{n - N_x}{(1 - p)^2} \middle\rvert p \right]
    &= \E\left[\frac{N_x}{p^2} \middle\rvert p\right] + \E\left[\frac{n - N_x}{(1 - p)^2} \middle\rvert p \right] \\
    &= \frac{n p}{p^2} + \frac{n (1 - p)}{(1 - p)^2}\\
    &= \frac{n}{p} + \frac{n}{1 - p} \\
    &= \frac{n}{p(1 - p)}.
\end{align*}
\section*{1 (c)}
Asymptotically Normal with mean XX and var XX.
\section*{1 (d)}
\section*{1 (e)}
\section*{1 (f)}

\section*{2 (a)}
\section*{2 (b)}
\section*{2 (c)}

\section*{3 (a)}
The second derivative of $f(x)$ is given by, \[\frac{d^2 f}{d x^2} = -\frac{1}{x^2}.\] This new function
is negative at all points other than 0, hence $f$ is concave.
\section*{3 (b)}
The second derivative of $f(x)$, \[\frac{d^2 f}{d x^2} = \frac{\exp(x)(1 - \exp(x))}{(1 +
\exp(x))^3}.\] For values $x \leq 0$ then $f(x)$ will be non-negative; however, when $x > 0$
$f(x)$ will be negative due to the $1 - \exp(x)$ term. Thus $f$ is neither concave nor convex.

\section*{4 (a)}
The RSS can be written as norm of a linear function, which is the composition of two convex
functions and hence is convex. However we will show more directly by second-order conditions.
We define RSS using vector/matrix notation as, \[RSS(\beta) = (y - X\beta)^t(y - X\beta)\] whose
second derivative is given by,
\begin{align*}
    \frac{\partial RSS^2}{\partial \beta \partial \beta} = 2X^t X.
\end{align*}
Now, RSS is convex iff $x^tX^tXx \geq 0$ for any $x \in \mathbb{R}^n$. Let $Xx \triangleq u$, then
$x^tX^tXx = u^t u = \sum_i u_i^2 \geq 0,$ hence RSS is convex.
\section*{4 (b)}
All the expectations are conditional on $X$, however I've dropped them for notational simplicity.
\begin{align*}
    \text{bias}(\hat{\beta}) &= \E[\hat{\beta} - \beta] = \E[(X^tX)^{-1}X^ty - \beta] = \E[(X^tX)^{-1}X^t(X\beta - \varepsilon)] - \beta \\
    &= (X^tX)^{-1}X^tX\beta - (X^tX)^{-1}X^t\E[\varepsilon] - \beta = \beta - 0 - \beta  = 0.
\end{align*}
Before performing our variance calculations first note from our above calclations that
$\hat{\beta} - \beta = (X^tX)^{-1}X^t \varepsilon.$ Now we can compute variance of our estimator
$\hat{\beta}$ as,
\begin{align*}
    \Var(\hat{\beta}) & = \Var(\hat{\beta} - \beta) = \Var((X^tX)^{-1}X^t \varepsilon) \\
    & = (X^tX)^{-1}X^t \Var(\varepsilon) ((X^tX)^{-1}X^t)^t =  (X^tX)^{-1}X^t \E(\varepsilon \varepsilon^t) ((X^tX)^{-1}X^t)^t \\
    & = (X^tX)^{-1}X^t \sigma^2 I_n ((X^tX)^{-1}X^t)^t  = \sigma^2(X^tX)^{-1}
\end{align*}
\section*{4 (c)}
\section*{4 (d)}

\section*{5 (a)}
\section*{5 (b)}
\section*{5 (c)}
\section*{5 (d)}

\end{document}
